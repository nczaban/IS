% !TEX root = ../IS.tex
\chapter{Machine Learning}
\section{Decision Trees}
One common machine learning technique is the decision tree. It can approximate discrete-valued functions \cite{mitc97} and automatically classify data\cite{sega07}. Generally, decision trees focus on problems with attribute-value pairs; an attribute \textit{Temperature} may have the possible values \textit{ Hot, Cold, Warm, Cool} \cite{mitc97}.

\subsection{ID3 Algorithm}
The ID3 algorithm for decision trees, like most decision tree algorithms, uses a top-down greedy search \cite{mitc97}. The algorithm begins by evaluating the input data by individual attributes in the training set. For each attribute, the algorithm attempts to classify the entire data set using only that attribute. Whichever attribute was the most successful on its own becomes the root of the tree. Then, for each possible value of that root attribute, a descendant node is created and the training set is split accordingly. Then, for each descendant node and its respective subset of training data, the process is repeated.\\

To find the best attribute at each node of the tree, the ID3 algorithm uses the statistical property information gain. Information gain utilizes entropy, or the relative impurity of a data collection.
\begin{define}
  For a set of training data $S$ with $n$ different target values, Entropy(S)$\equiv\sum_{i=1}^n-p_i$log$_2p_i$, where $p_i$ is the proportion of S with a target value of $i$.
\end{define}
Entropy calculations define 0log(0) as 0. The logarithm can have any base value, but two is often used; a base of 2 conveys the expected encoding length in bits \cite{mitc97}. Assuming the base value is 2, an entropy value of 0 occurs when all data set elements have the same target value $i$, and a  set with a 50/50 split between two target values will have an entropy value of 1. Information gain is defined as the expected reduction in entropy caused by splitting the training data by a particular attribute.
\begin{define}
  For an attribute $A$ and a set of training data $S$, the Information Gain(S, A)$\equiv$ Entropy(S) $-\sum_{v\in Values(A)}\frac{|S_v|}{|S|}$ Entropy$(S_v)$, where $Values(A)$ is the set of possible values for the attribute $A$, and $S_v$ is the subset of $S$ where $A$ has value $v$.
\end{define}
Each attribute $A$ for a given node is tested, with each of its possible values used in the summation. For these values, the entropy of each subset $S_v$ with is calculated, then scaled by the size of $S_v$ compared to $S$.

\begin{algorithm}
  \caption{ID3 Decision Tree Algorithm}
  \begin{algorithmic}
    \Procedure {ID3}{TrainingSet $S$, TargetAttribute $t$, AttributeSet $A$}
    \State Create node $Root$
    \State $TestEntropy\gets True$
    \State $TestAttributeValue\gets S_0$
    \For{$i\in S$}
    \Comment Check whether the TrainingSet $S$ is uniform on TargetAttribute $t$
    \If{the TargetAttribute of $i=TestAttributeValue$}
    \State Pass
    \Else
    \State $TestEntropy\gets False$
    \State Break
    \Comment One of the elements of $S$ has a different value for $t$, exit the for loop
    \EndIf
    \EndFor
    \If{$TestEntropy == True$}
    \State Return $Root$ with label $v$\\
    \Comment Every element in $S$ has the same value for $t$, this is a single-node tree
    \ElsIf{AttributeSet $A$ is the empty set}
    \State Return $Root$ with label most common value of $t$ in TrainingSet\\
    \Comment All attributes have been examined and $S$ cannot be split further
    \Else
    \State $OptimalInformationGain=0$
    \For{all $a\in$AttributeSet}
    \Comment Determine optimal split for the set $S$
    \State $InformationGain \gets Entropy(S)$
    \State $EntropySum\gets 0$
    \For{all values $v\in a$}
    \State $S_v \gets \{S:Value(a)=v\}$
    \Comment $S_v$ contains elements of $S$ which have TargetAttribute $v$
    \State $EntropySum\gets EntropySum + \frac{S_v}{S} * Entropy(S_v)$\\
    \Comment Add the entropy for each possible value $v$ of $a$.
    \EndFor
    \State $InformationGain \gets InformationGain - EntropySum$
    \Comment This is the information gain from splitting the set along $a$.
    \If{$InformationGain > OptimalInformationGain$}
    
    \EndIf
    \EndFor
    \EndIf
    \State Return $Root$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Once a tree has been created with the algorithm, it is simple to classify new data. Starting at the root of the tree, each target attribute is tested for recursively, moving from one node to another down the tree \cite{sega07}.\\

\subsection{Optimizing the Tree}
Decision trees are susceptible to overfitting, where decisions at each node are too closely tied to the training set. Overfitting is caused by an algorithm creating superfluous branches which slightly decrease entropy for the set, but are in fact arbitrary decisions \cite{sega07}. For example, a decision tree could be created to classify students to their fields of study. An overfitted tree might contain choice nodes checking the names of the students. The tree will be able to reach 0 entropy on the training set, knowing that ``John Smith'' is an English major and ``Jane Doe'' is a Chemistry major, but this tree will not be able to correctly classify a ``John Doe'' or ``Jane Smith.''\\

There are two common methods to prevent overfitting. The first method is to require a minimum reduction in entropy at each split. Once the minimum is not reached, the tree stops creating additional branches. While commonly used, this method is insufficient in cases where early splits do not reduce entropy, but later splits will greatly reduce the entropy of the set \cite{sega07}. The second method is to build the tree, then prune any extraneous nodes. Working from the bottom-up, the tree examines two leaf nodes with the same parent and determines whether merging these nodes would not increase entropy more than a specified threshold. If the nodes pass this test, they are combined and their parent becomes a new leaf node.\\

